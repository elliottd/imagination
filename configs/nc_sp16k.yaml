model: nmt
mode: train
output_dir: nmt/models/nc_sp16k_aux_
model_name: nc_baseline
reload: False

# Data
src_train: nmt/data/sentencepiece16k/news_commentary/train.en.norm.tok.lower.sp
trg_train: nmt/data/sentencepiece16k/news_commentary/train.de.norm.tok.lower.sp
src_valid: nmt/data/sentencepiece16k/news_commentary/dev.en.norm.tok.lower.sp
trg_valid: nmt/data/sentencepiece16k/news_commentary/dev.de.norm.tok.lower.sp
src_dicts: [nmt/data/sentencepiece16k/news_commentary/joint.all.norm.tok.lower.sp.json]
trg_dicts: [nmt/data/sentencepiece16k/news_commentary/joint.all.norm.tok.lower.sp.json]
n_words_src: 17597
n_words_trg: 17597
eos_symbol: </s>
unk_symbol: <UNK>

# Model hyperparameters
decoder: gru
disable_attention: False
dim_emb: 620
dim_per_factor: [620]
factors: 1
factors_trg: 1
dim: 1000
dim_att: 2000
dropout: True
dropout_src: 0.0
dropout_emb: 0.2
dropout_rec: 0.2
dropout_hid: 0.0
beam_size: 12

# Training hyperparameters
batch_size: 80
sort_k_batches: 3
maxlen: 120
max_epochs: 100
optimizer: adam
learning_rate: 1e-4
decay_c: 0.
clip_c: 1.
alpha_c: 0.
early_stopping: bleu
patience: 5
subword_at_replace: True
at_replace: False
bleu_val_out: validation
bleu_val_burnin: 100000
bleu_val_ref: nmt/data/sentencepiece16k/news_commentary/dev.de.norm.tok.lower
bleu_script: nmt/nmt/multi-bleu.perl
postprocess_script: "cat"
save_frequency: -1
validation_frequency: -1
sample_frequency: -1
finish_after: 1000000
track_n_models: 3

# Feedback
display_frequency: 100
verbose: null
disp_alignments: False

# Multi-tasking
mtl: False
mtl_ratio: [0.5, 0.5]
mtl_configs: ['bpe.multi30k.inceptionv3.yaml']
n_shared_layers: 1

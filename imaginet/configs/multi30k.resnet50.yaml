model: imaginet
mode: imaginet
model_name: multi30k-resnet50-w620-h1000-meanmlp-tanh-constrastive0.1-adam-5e-5-dropout_hid0.3-decay_c1e-8-alltoks.npz # the name of the file in which we'll save the weights
config_name: multi30k.resnet50.yaml # the name of the configuration file
output_dir: imaginet/checkpoints/ # where to save the model weights
word_vocabulary: nmt/data/wmt_task1/en_dict.json # where is the vocabulary file?

# input data for text files, image vectors, and wordsims
train: nmt/data/wmt_task1/train.en
fc7_train: nmt/data/wmt_task1/train-resnet50.hdf5
val: nmt/data/wmt_task1/dev.en
fc7_val: nmt/data/wmt_task1/val-resnet50.hdf5
sim_file: imaginet/data/simlex-999/EN-SimLex-999.txt

# text data
eos_symbol: '</s>'        # end of sentence symbol
unk_symbol: '<UNK>'       # unknown word symbol
max_words: -1             # UNK replace words beyond here
max_unk_ratio: 1.0

# model hyperparameters
dim: 1000
dim_emb: 620
dim_per_factor: [620]
dim_v: 2048
encoder_layers: 1         # if you want a deep RNN
learn_rnn_init: True     # if you want to learn the initial state of the RNN
init: ortho
final_birnn: False
mean_birnn: True

# dropout
dropout: True             # True or False (True => tag.test_value will break)
dropout_word: 0.0         # dropout whole words
dropout_emb: 0.2          # dropout parts of the embeddings
dropout_rec: 0.2          # dropout recurrent connections
dropout_hid: 0.0          # dropout on hidden to output states (in the MLP)

# MLP
activation_mlp: tanh      # relu or tanh activation

# training
loss: constrastive
margin: 0.1
batch_size: 80
num_epochs: 100
optimizer: adam
lr: 0.0001
clip_c: 1.0              # clip gradients at this value
decay_c: 0.              # L2 regularisation term
verbose: False
compute_test_values: 'off'    # 'warn' turns it on
validation_frequency: -1
display_frequency: 100
reload: False

early_stopping: medr     # medr or corr
patience: 5              # number of epochs to delay early stopping
ranking_k: 1             # number of possibly relevant matches when ranking
